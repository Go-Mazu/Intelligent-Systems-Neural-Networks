# Intelligent-Systems-Neural-Networks

The objective was to develop an understanding of neural networks and fitting a model with data splitting for train, validation, and test sets. The 2 activation functions relu and tanh competed in performance across the different conditions, but each function has its own advantages and appropriate use cases, personally I prefer using Relu because we don't have to deal with the vanishing gradient problem. The results reflect the importance of using shuffled data and appropriately balancing the number of layers and neurons used.
